#+title: epydemic
#+startup: content

* epydemic: epidemic simulation in Python                           :PROJECT:

** Release planning

*** Release 1.6.1

**** Coding [1/6]

    - [ ] Configuration model generator, with both a list of node
      degrees (as in networkx) and a distribution to draw from
    - [ ] Extended configuration model with triangles etc
    - [ ] Add degree-distribution-preserving rewiring functions to
      randomise networks while preserving p_k or P(k, k')
      cite:UnreasonableEffectiveness
    - [X] Add flag to parameters about the network topology used
      (branch topology-flag)

**** Bug fixes [0/0]

**** Documentation [0/1]

     - [ ] Add discussion of moving from R-values to parameters


*** Release 1.5.1                                                   :ARCHIVE:

**** Coding [1/1]

     - [X] Integrate accelerated simulation ([[*Improving sequential Gillespie simulation][below]])

**** Bug fixes [1/1]

     - [X] Problem with monitor cookbook recipe code

**** Documentation [1/1]

     - [X] Add documentation for DrawSet


** Sub-projects

*** Acceleration

**** numba acceleration                                             :ARCHIVE:

 git branch numba-acceleration

 Idea: wrap StochasticDynamics.do() as a JIT-compiled function, since
 that (and the event functions) are where most of the time is spent. If
 it generate worthwhile speed-up, extend out to other elements that are
 time-consuming.

 The main simulation loop seems like a good place to start as it
 involves a lot of looping and drawing from probability distributions,w
 which should be accelerable.

 Installing the latest numba (0.51.2) installs llvmlite-0.34.0, which
 only works for versions of LLVM up to 10.0.x. The latest arch version
 is 11.x, so I downgraded to the latest compatible version (and also
 its libraries):

 #+BEGIN_SRC sh
   pacman -U https://archive.archlinux.org/packages/l/llvm/llvm-10.0.1-3-x86_64.pkg.tar.zst
   pacman -U https://archive.archlinux.org/packages/l/llvm10-libs/llvm10-libs-10.0.1-3-x86_64.pkg.tar.zst
 #+END_SRC

 Doesn't seem to get much speed-up, even given it's quite numerical:
 there are calls to get the event distribution and to check for
 equilibrium that perhaps could be refactored?

**** GPU acceleration

 Will need to be [[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html][containerised]].

*** PyPy-based compute cluster controlled from CPython [0/2]

    branch pypy-compute

    PyPy doesn't have a full set of library integrations: specifically
    it can't load scipy or matplotlib. That (especially the latter)
    makes it unsuitable for front-end use a lot of the time.

    One way to address this is to run a compute cluster in a PyPy venv
    and control it from Jupyter running in a CPython venv. That way we
    can get the acceleration without the problems.

    Of course this opens up another can of worms. The latest CPython
    is version 3.9; the latest PyPy is on 3.7. So we'd have to run an
    older CPython to avoid language mis-matches.

    - [ ] Set up compute cluster in PyPy linked to Jupyter on CPython
    - [ ] Check interactions between versions

*** Containerisation

    To run in the cloud we need to be able to containerise. There are a
    couple of options here:

    1. A single container running on a multicore host, extended with
       whatever code is needed for the application. This is
       straightforward, but limited by the single-host performance
       (which might be fine for a lot of applications).
    2. Multiple containers acting together, with a virtual network
       between them. This probably needs ~docker-compose~ and some
       tests to see whether it's possible to run ~ipyparallel~ in this
       way (which I think it is).

*** Generating function library [1/4]

    branch gf

    We need a generating functions library, perhaps alongside the
    network generator classes, so we can use this formalism easily
    alongside epydemic's simulations. In particular we need the
    high-order-numerical-derivative function to be able to extract
    probabilities etc.

    The biggest challenge might be to write documentation....

    There's another approach alongside this, which would be to write a
    symbolic package with the generating functions in them, for use in
    Sage. This would then complement the numerical side.

    - [X] Basic functions and derivatives

    There are several ways we might want to specify a GF:

    - as a single function that captures the entire series in one
      expression
    - as a sequence of coefficient literals, for example extracted
      from the degree histogram of a network
    - as a function that takes the degree of a term in the GF and
      returns the corresponding coefficient
    - As a function that takes the degree and the current coefficient
      and returns a new one, for deriving GFs from one another
    - as a derivative to some order of another GF
    - as a composition of underlying GFs to get GFs of several
      variables

    All feel like they'd be useful for the applications you find in
    the literature.

    - [ ] Different coefficient specifiers
    - [ ] Combinators
    - [ ] Examples of all the above, for tests?

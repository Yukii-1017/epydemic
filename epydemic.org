#+title: epydemic

* epydemic, epidemic simulation in Python                           :PROJECT:

** Release planning

*** Release 1.7.1

**** Coding [0/3]

    - [ ] Extended configuration model with triangles etc
    - [ ] Add degree-distribution-preserving rewiring functions to
      randomise networks while preserving p_k or P(k, k')
      cite:UnreasonableEffectiveness
    - [ ] Configuration model generator, with both a list of node
      degrees (as in networkx) and a distribution to draw from (need
      to avoid large parameter lists))

*** Release 1.6.1

**** Coding [1/1]

     - [X] Change topology parameters and markers so we can retrieve
       them

**** Documentation [1/1]

     - [X] Add discussion of moving from R-values to parameters

*** Release 1.5.1                                                   :ARCHIVE:

**** Coding [1/1]

     - [X] Integrate accelerated simulation ([[*Improving sequential Gillespie simulation][below]])

**** Bug fixes [1/1]

     - [X] Problem with monitor cookbook recipe code

**** Documentation [1/1]

     - [X] Add documentation for DrawSet


** Sub-projects

*** Acceleration

**** numba acceleration                                             :ARCHIVE:

 git branch numba-acceleration

 Idea: wrap StochasticDynamics.do() as a JIT-compiled function, since
 that (and the event functions) are where most of the time is spent. If
 it generate worthwhile speed-up, extend out to other elements that are
 time-consuming.

 The main simulation loop seems like a good place to start as it
 involves a lot of looping and drawing from probability distributions,w
 which should be accelerable.

 Installing the latest numba (0.51.2) installs llvmlite-0.34.0, which
 only works for versions of LLVM up to 10.0.x. The latest arch version
 is 11.x, so I downgraded to the latest compatible version (and also
 its libraries):

 #+BEGIN_SRC sh
   pacman -U https://archive.archlinux.org/packages/l/llvm/llvm-10.0.1-3-x86_64.pkg.tar.zst
   pacman -U https://archive.archlinux.org/packages/l/llvm10-libs/llvm10-libs-10.0.1-3-x86_64.pkg.tar.zst
 #+END_SRC

 Doesn't seem to get much speed-up, even given it's quite numerical:
 there are calls to get the event distribution and to check for
 equilibrium that perhaps could be refactored?

**** GPU acceleration

 Will need to be [[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html][containerised]].

**** Cython acceleration

     Using Cython requires code changes. They're only annotations, to
     provide C types for variables and calling conventions on methods
     -- but fairly substantial changes, and not backwards compatible,
     meaning it'd be a commitment

*** Containerisation

    To run in the cloud we need to be able to containerise. There are a
    couple of options here:

    1. A single container running on a multicore host, extended with
       whatever code is needed for the application. This is
       straightforward, but limited by the single-host performance
       (which might be fine for a lot of applications).
    2. Multiple containers acting together, with a virtual network
       between them. This probably needs ~docker-compose~ and some
       tests to see whether it's possible to run ~ipyparallel~ in this
       way (which I think it is).

*** Generating function library

    We need a generating functions library, perhaps alongside the
    network generator classes, so we can use this formalism easily
    alongside epydemic's simulations. In particular we need the
    high-order-numerical-derivative function to be able to extract
    probabilities etc.

    The biggest challenge might be to write documentation....

    There's another approach alongside this, which would be to write a
    symbolic package with the generating functions in them, for use in
    Sage. This would then complement the numerical side.

#+title: epydemic
#+startup: content

* epydemic: Epidemic simulation in Python                           :PROJECT:

** Release planning

*** Release 1.7.1

**** Coding [0/1]

    - [ ] Add degree-distribution-preserving rewiring functions to
      randomise networks while preserving p_k or P(k, k')
      cite:UnreasonableEffectiveness
    - [X] Add flag to parameters about the network topology used
      (branch topology-flag)

*** Release 1.6.1                                                   :ARCHIVE:

**** Coding [1/1]

     - [X] Change topology parameters and markers so we can retrieve
       them

**** Documentation [1/1]

     - [X] Add discussion of moving from R-values to parameters

*** Release 1.5.1                                                   :ARCHIVE:

**** Coding [1/1]

     - [X] Integrate accelerated simulation ([[*Improving sequential Gillespie simulation][below]])

**** Bug fixes [1/1]

     - [X] Problem with monitor cookbook recipe code

**** Documentation [1/1]

     - [X] Add documentation for DrawSet


** Sub-projects

*** Acceleration

**** numba acceleration                                             :ARCHIVE:

 git branch numba-acceleration

 Idea: wrap StochasticDynamics.do() as a JIT-compiled function, since
 that (and the event functions) are where most of the time is spent. If
 it generate worthwhile speed-up, extend out to other elements that are
 time-consuming.

 The main simulation loop seems like a good place to start as it
 involves a lot of looping and drawing from probability distributions,w
 which should be accelerable.

 Installing the latest numba (0.51.2) installs llvmlite-0.34.0, which
 only works for versions of LLVM up to 10.0.x. The latest arch version
 is 11.x, so I downgraded to the latest compatible version (and also
 its libraries):

 #+BEGIN_SRC sh
   pacman -U https://archive.archlinux.org/packages/l/llvm/llvm-10.0.1-3-x86_64.pkg.tar.zst
   pacman -U https://archive.archlinux.org/packages/l/llvm10-libs/llvm10-libs-10.0.1-3-x86_64.pkg.tar.zst
 #+END_SRC

 Doesn't seem to get much speed-up, even given it's quite numerical:
 there are calls to get the event distribution and to check for
 equilibrium that perhaps could be refactored?

**** GPU acceleration

 Will need to be [[https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/overview.html][containerised]].

**** Cython acceleration

     Using Cython requires code changes. They're only annotations, to
     provide C types for variables and calling conventions on methods
     -- but fairly substantial changes, and not backwards compatible,
     meaning it'd be a commitment

     Also compare against [[https://github.com/pyston/pyston][Pyston]]

*** PyPy-based compute cluster controlled from CPython [0/2]

    branch pypy-compute

    PyPy doesn't have a full set of library integrations: specifically
    it can't load scipy or matplotlib. That (especially the latter)
    makes it unsuitable for front-end use a lot of the time.

    One way to address this is to run a compute cluster in a PyPy venv
    and control it from Jupyter running in a CPython venv. That way we
    can get the acceleration without the problems.

    Of course this opens up another can of worms. The latest CPython
    is version 3.9; the latest PyPy is on 3.7. So we'd have to run an
    older CPython to avoid language mis-matches.

    - [ ] Set up compute cluster in PyPy linked to Jupyter on CPython
    - [ ] Check interactions between versions


*** Containerisation

    To run in the cloud we need to be able to containerise. There are a
    couple of options here:

    1. A single container running on a multicore host, extended with
       whatever code is needed for the application. This is
       straightforward, but limited by the single-host performance
       (which might be fine for a lot of applications).
    2. Multiple containers acting together, with a virtual network
       between them. This probably needs ~docker-compose~ and some
       tests to see whether it's possible to run ~ipyparallel~ in this
       way (which I think it is).

*** Generating function library [0/4]

    branch gf

    We need a generating functions library, perhaps alongside the
    network generator classes, so we can use this formalism easily
    alongside epydemic's simulations. In particular we need the
    high-order-numerical-derivative function to be able to extract
    probabilities etc.

    The biggest challenge might be to write documentation....

    There's another approach alongside this, which would be to write a
    symbolic package with the generating functions in them, for use in
    Sage. This would then complement the numerical side.

    - [ ] First version
    - [ ] Integrate with calls etc, so they behave like functions
    - [ ] Access coefficients by index
    - [ ] High-degree (k=1000) coefficient extraction


*** Paper on draw set implementation

**** TODO Complete analysis of fairness
     SCHEDULED: <2021-06-04 Fri>
**** TODO Ask Len Thomas about other statistical tests
**** TODO Numerical exploration
**** TODO First draft
